<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="true" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>MLP | Justin的技术博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="在 Embedding 里 dim 的意思dim 表示你操作的 维度索引。张量有很多维度（axis），dim 决定了你在哪个维度上做操作。 举例：   1emb.shape &#x3D; (32, 3, 2)  这里 3 个维度的含义分别是：  dim&#x3D;0 → batch 维度（32 个样本）  dim&#x3D;1 → 序列长度（每个样本 3 个索引）  dim&#x3D;2 → embedding 向量维度（每个索引是 2">
<meta property="og:type" content="article">
<meta property="og:title" content="MLP">
<meta property="og:url" content="https://jayli19707.github.io/2025/08/26/MLP/index.html">
<meta property="og:site_name" content="Justin的技术博客">
<meta property="og:description" content="在 Embedding 里 dim 的意思dim 表示你操作的 维度索引。张量有很多维度（axis），dim 决定了你在哪个维度上做操作。 举例：   1emb.shape &#x3D; (32, 3, 2)  这里 3 个维度的含义分别是：  dim&#x3D;0 → batch 维度（32 个样本）  dim&#x3D;1 → 序列长度（每个样本 3 个索引）  dim&#x3D;2 → embedding 向量维度（每个索引是 2">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250824053243517.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826080634031.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826080722214.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826080319533.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826081534156.png">
<meta property="article:published_time" content="2025-08-26T00:20:54.000Z">
<meta property="article:modified_time" content="2025-08-26T00:21:00.576Z">
<meta property="article:author" content="Justin">
<meta property="article:tag" content="人工智能,机器学习,深度学习,量化交易,Python,技术博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250824053243517.png">
  
    <link rel="alternate" href="/atom.xml" title="Justin的技术博客" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  
  
    
<div id="banner" class="">
  <img src="/image/background/benjamin_background.jpg" itemprop="image">
  <div id="banner-dim"></div>
</div>
 
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>Justin的技术博客 </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/projects">Projects</a>
    
      <a class="main-nav-link" href="/investment">Investment</a>
    
      <a class="main-nav-link" href="/photography">Photography</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/projects">Projects</a>
    
      <a class="nav-dropdown-link" href="/investment">Investment</a>
    
      <a class="nav-dropdown-link" href="/photography">Photography</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/image/avatar/avatar_1.png></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">Justin </div>
      <div class="dot"></div>
      <div class="subtitle">比世界先发现你发光 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/JAYLI19707" title="GitHub"><i class="fa-brands fa-github"></i></a>
        
          <a class="link-btn" href="mailto:your.email@gmail.com" title="Email"><i class="fa-solid fa-envelope"></i></a>
        
          <a class="link-btn" href="/atom.xml" title="RSS"><i class="fa-solid fa-rss"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">
                计算机
                <div class="category-count">4</div>
            </a>
        
            <a class="category-link" href="/categories/Leetcode/">
                Leetcode
                <div class="category-count">19</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">
                开发工具
                <div class="category-count">1</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                机器学习
                <div class="category-count">19</div>
            </a>
        
            <a class="category-link" href="/categories/%E9%87%91%E8%9E%8D/">
                金融
                <div class="category-count">8</div>
            </a>
        
            <a class="category-link" href="/categories/Quant/">
                Quant
                <div class="category-count">2</div>
            </a>
        
            <a class="category-link" href="/categories/AI/">
                AI
                <div class="category-count">3</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">
                操作系统
                <div class="category-count">1</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%95%B0%E5%AD%A6/">
                数学
                <div class="category-count">8</div>
            </a>
        
            <a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">
                计算机网络
                <div class="category-count">1</div>
            </a>
        
            <a class="category-link" href="/categories/%E7%AE%97%E6%B3%95/">
                算法
                <div class="category-count">1</div>
            </a>
        </div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">标签</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Batch/" rel="tag">Batch</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/LSTM/" rel="tag">LSTM</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Quant/" rel="tag">Quant</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Shell/" rel="tag">Shell</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E4%B8%B4%E8%BF%91%E7%AE%97%E5%AD%90/" rel="tag">临近算子</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E7%B1%BB/" rel="tag">二分类</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%87%B8%E5%87%BD%E6%95%B0/" rel="tag">凸函数</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/" rel="tag">最大似然估计</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%9F%E6%9D%83/" rel="tag">期权</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">梯度下降</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/" rel="tag">矩阵乘法</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/" rel="tag">统计假设检验</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" rel="tag">计算机网络</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%87%8F%E4%BB%B7%E8%B6%8B%E5%8A%BF/" rel="tag">量价趋势</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6/" rel="tag">金融数学</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" rel="tag">面试题</a></li></ul>
    </div>
  </div>


    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-MLP" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        MLP
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-08-26T00:20:54.000Z" itemprop="datePublished">2025-08-26</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            9k 词 
          </div>
        </div>
        
      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h1 id="在-Embedding-里-dim-的意思"><a href="#在-Embedding-里-dim-的意思" class="headerlink" title="在 Embedding 里 dim 的意思"></a>在 Embedding 里 <code>dim</code> 的意思</h1><p><code>dim</code> 表示你操作的 <strong>维度索引</strong>。<br>张量有很多维度（axis），<code>dim</code> 决定了你在哪个维度上做操作。</p>
<p>举例：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emb.shape = (<span class="number">32</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>这里 3 个维度的含义分别是：</p>
<ul>
<li><p><strong>dim=0</strong> → batch 维度（32 个样本）</p>
</li>
<li><p><strong>dim=1</strong> → 序列长度（每个样本 3 个索引）</p>
</li>
<li><p><strong>dim=2</strong> → embedding 向量维度（每个索引是 2 维）</p>
</li>
</ul>
<hr>
<h1 id="Embedding-查表-Embedding-Lookup"><a href="#Embedding-查表-Embedding-Lookup" class="headerlink" title="Embedding 查表 (Embedding Lookup)"></a>Embedding 查表 (Embedding Lookup)</h1><h2 id="思路拆解"><a href="#思路拆解" class="headerlink" title="思路拆解"></a>思路拆解</h2><ol>
<li><p><strong>X 的形状</strong></p>
<ul>
<li><p><code>X.shape = (32, 3)</code></p>
</li>
<li><p>含义：一共有 32 个样本（batch），每个样本里面有 3 个索引。</p>
</li>
</ul>
</li>
<li><p><strong>C 的形状</strong></p>
<ul>
<li><p><code>C.shape = (27, 2)</code></p>
</li>
<li><p>含义：这是一个查找表（embedding table），一共有 27 行，每行是 2 维向量。</p>
</li>
</ul>
</li>
<li><p><strong>C[X] 的运算</strong></p>
<ul>
<li><p>对每个 <code>X[i, j]</code>，用它作为索引，去 <code>C</code> 里取对应的那一行（形状 <code>(2,)</code>）。</p>
</li>
<li><p>所以一行 X <code>[a, b, c]</code> 会变成 <code>C[[a, b, c]]</code>，形状 <code>(3, 2)</code>。</p>
</li>
<li><p>整个 32 个样本就堆叠起来，得到 <code>(32, 3, 2)</code>。</p>
</li>
</ul>
</li>
</ol>
<h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="80.09ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 35400 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="merror" data-mjx-error="'_' allowed only in math mode" title="'_' allowed only in math mode"><rect data-background="true" width="35400" height="950" y="-200"></rect><title>'_' allowed only in math mode</title><g data-mml-node="mtext" style="font-family: serif;"><text data-variant="-explicitFont" transform="scale(1,-1)" font-size="884px">\text{shape}(C[X]) = \text{shape}(X) + \text{embedding_dim}</text></g></g></g></g></svg></mjx-container></p>
<p>在你的例子里：</p>
<p>(32,3)+(2,)=(32,3,2)(32, 3) + (2,) = (32, 3, 2)</p>
<hr>
<h1 id="拼接方式"><a href="#拼接方式" class="headerlink" title="拼接方式"></a>拼接方式</h1><h2 id="1-展平（flatten）"><a href="#1-展平（flatten）" class="headerlink" title="1. 展平（flatten）"></a>1. 展平（flatten）</h2><p>最高效的拼接方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emb.view(<span class="number">32</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>

<p>因为 <code>(32, 3, 2)</code> 可以理解为 3×2=6，所以把最后两维拼起来，得到</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emb.view(32, 6).shape = (32, 6)</span><br></pre></td></tr></table></figure>

<p>每个样本的 embedding 被拉平成一个长度为 6 的向量。</p>
<h2 id="2-手动切片再拼接"><a href="#2-手动切片再拼接" class="headerlink" title="2.手动切片再拼接"></a>2.手动切片再拼接</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([emb[:, <span class="number">0</span>, :], emb[:, <span class="number">1</span>, :], emb[:, <span class="number">2</span>, :]], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>emb[:, 0, :]</code> → (32, 2)</p>
</li>
<li><p><code>emb[:, 1, :]</code> → (32, 2)</p>
</li>
<li><p><code>emb[:, 2, :]</code> → (32, 2)<br>  拼接后 <code>(32, 6)</code>。</p>
</li>
</ul>
<h2 id="3-利用-unbind"><a href="#3-利用-unbind" class="headerlink" title="3.利用 unbind"></a>3.利用 unbind</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(torch.unbind(emb, dim=<span class="number">1</span>), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>torch.unbind(emb, dim=1)</code> 会把第 1 维（长度=3）拆开 → 得到 3 个 <code>(32, 2)</code> 张量</p>
</li>
<li><p>再 <code>torch.cat(..., dim=1)</code> 拼接 → <code>(32, 6)</code></p>
</li>
</ul>
<p>两种方法都是 <strong>把 (32,3,2) 转成 (32,6)</strong>，本质上等价于 <code>emb.view(32,6)</code>。</p>
<h1 id="项目拆解："><a href="#项目拆解：" class="headerlink" title="项目拆解："></a>项目拆解：</h1><p><img src="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250824053243517.png" alt="image.png"></p>
<p>模型的输入和输出维度是变化的，我们来一步步拆解一下。</p>
<p>简单来说，可以把数据在网络中的流动想象成一个工厂流水线，原材料（输入）在每个工位（网络层）都会被加工，其“形状”（维度）也会随之改变。</p>
<p>我们用视频中的具体例子来解释：</p>
<ul>
<li><p><code>batch_size</code> (批量大小) = 32</p>
</li>
<li><p><code>block_size</code> (上下文长度) = 3</p>
</li>
<li><p><code>embedding_dim</code> (嵌入向量维度) = 10</p>
</li>
<li><p><code>vocab_size</code> (词汇表大小) = 27</p>
</li>
<li><p><code>hidden_size</code> (隐藏层神经元数量) = 200</p>
</li>
</ul>
<hr>
<h3 id="输入-Input-的维度变化"><a href="#输入-Input-的维度变化" class="headerlink" title="输入 (Input) 的维度变化"></a><strong>输入 (Input) 的维度变化</strong></h3><p>输入的处理过程主要有三步，每一步的维度都不同：</p>
<ol>
<li><p><strong>最原始的输入 <code>X</code>：(32, 3)</strong></p>
<ul>
<li><p><strong>维度</strong>: <code>(batch_size, block_size)</code></p>
</li>
<li><p><strong>解释</strong>: 这是我们喂给模型的<strong>第一批原始数据</strong>。它是一个整数张量（Tensor）。每一行是一个训练样本（共32个），每一行中的3个数字是构成上下文的3个字符的<strong>索引</strong>（例如，<code>[5, 13, 13]</code> 代表 ‘e’, ‘m’, ‘m’）。此时，数据还没有“特征”的概念，只是代表字符的编号。</p>
</li>
</ul>
</li>
<li><p><strong>经过嵌入层后的输入 <code>emb</code>：(32, 3, 10)</strong></p>
<ul>
<li><p><strong>维度</strong>: <code>(batch_size, block_size, embedding_dim)</code></p>
</li>
<li><p><strong>解释</strong>: 这是<strong>真正进入神经网络计算的特征</strong>。模型通过一个查找表 <code>C</code>（形状为 <code>(27, 10)</code>），将上一步的每个字符索引（如 <code>5</code>）都转换成了一个10维的特征向量（词嵌入）。所以，原来 <code>(32, 3)</code> 的索引矩阵，就变成了 <code>(32, 3, 10)</code> 的浮点数特征矩阵。现在每一行代表一个样本，每个样本包含3个10维的向量。</p>
</li>
</ul>
</li>
<li><p><strong>展平（Flatten）后的输入：(32, 30)</strong></p>
<ul>
<li><p><strong>维度</strong>: <code>(batch_size, block_size * embedding_dim)</code></p>
</li>
<li><p><strong>解释</strong>: 全连接的隐藏层（Linear Layer）无法直接处理 <code>(32, 3, 10)</code> 这样的三维输入。它需要一个二维的矩阵，形状为 <code>(批量大小, 特征数量)</code>。因此，我们需要将每个样本的3个10维向量<strong>拼接</strong>成一个单一的30维向量。这一步就是通过 <code>torch.view</code> 操作完成的。<code>view(32, 30)</code> 将 <code>(32, 3, 10)</code> 的数据“压平”成了 <code>(32, 30)</code>。<strong>这才是隐藏层接收到的最终输入</strong>。</p>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="输出-Output-的维度"><a href="#输出-Output-的维度" class="headerlink" title="输出 (Output) 的维度"></a><strong>输出 (Output) 的维度</strong></h3><p>网络的最终输出，我们只关心其预测结果的维度。</p>
<ul>
<li><p><strong>最终输出 <code>logits</code>：(32, 27)</strong></p>
<ul>
<li><p><strong>维度</strong>: <code>(batch_size, vocab_size)</code></p>
</li>
<li><p><strong>解释</strong>: 经过隐藏层（输出 <code>(32, 200)</code>）和最后的输出层之后，模型为我们批量中的<strong>每一个样本</strong>（共32个），都生成了一个包含27个数值的向量。</p>
</li>
<li><p>这27个数值就是 <strong>logits</strong>（原始得分），分别对应着字母表中下一个字符可能是’a’到’z’以及特殊字符’.’的概率得分。这个得分越高，代表模型认为该字符是下一个正确字符的可能性越大。</p>
</li>
<li><p>这个 <code>(32, 27)</code> 的输出会和我们的<strong>目标标签 <code>Y</code></strong>（一个形状为 <code>(32,)</code> 的整数向量，包含了32个样本的正确答案索引）一起被送入 <code>F.cross_entropy</code> 损失函数中，用于计算损失并进行反向传播。</p>
</li>
</ul>
</li>
</ul>
<h1 id="完整流程："><a href="#完整流程：" class="headerlink" title="完整流程："></a>完整流程：</h1><h3 id="1-输入：确实是一整个“批次”的“索引矩阵”"><a href="#1-输入：确实是一整个“批次”的“索引矩阵”" class="headerlink" title="1. 输入：确实是一整个“批次”的“索引矩阵”"></a>1. 输入：确实是一整个“批次”的“索引矩阵”</h3><p>我们不是一个一个字符地输入，也不是一个一个样本（一组上下文）地输入，而是一次性输入一个**批次（batch）**的数据。</p>
<ul>
<li><p><strong>输入 <code>X</code> 的形状是 <code>(32, 3)</code></strong>：这代表我们一次性处理 <strong>32个不同的训练样本</strong>。</p>
</li>
<li><p><strong>每个样本的维度是 <code>3</code></strong>：这 <code>3</code> 个数字就是您说的“拼接好的字符索引”，也就是上下文窗口（<code>block_size</code>）中的3个字符在词汇表里的编号。</p>
</li>
</ul>
<h3 id="2-Embedding层：学习“关系”的压缩空间"><a href="#2-Embedding层：学习“关系”的压缩空间" class="headerlink" title="2. Embedding层：学习“关系”的压缩空间"></a>2. Embedding层：学习“关系”的压缩空间</h3><ul>
<li><p><strong>查找与转换</strong>：模型接收到 <code>(32, 3)</code> 的索引后，会去一个叫做 <code>C</code> 的大矩阵（图中的 “Matrix C”）里查找。这个 <code>C</code> 的形状是 <code>(27, 10)</code>，相当于一张有27行、10列的表格。</p>
</li>
<li><p><strong>学习相似性</strong>：输入的每个索引（比如<code>5</code>）就对应表格中的某一行（第5行），这一行就是一个10维的向量。在训练过程中，模型会不断调整这个 <code>C</code> 矩阵里的值，使得<strong>功能或意义上相似的字符</strong>（比如所有的元音字母）它们的向量在10维空间里的位置会变得越来越接近。</p>
</li>
</ul>
<h3 id="3-展平（Flatten）：为隐藏层准备“标准格式”的输入"><a href="#3-展平（Flatten）：为隐藏层准备“标准格式”的输入" class="headerlink" title="3. 展平（Flatten）：为隐藏层准备“标准格式”的输入"></a>3. 展平（Flatten）：为隐藏层准备“标准格式”的输入</h3><ul>
<li><p>经过 Embedding 后，我们的数据形状从 <code>(32, 3)</code> 变成了 <code>(32, 3, 10)</code>。</p>
</li>
<li><p>但是，标准的<strong>全连接隐藏层</strong>（图中标着 <code>tanh</code> 的那一层）只能接收一个二维矩阵 <code>(样本数, 特征数)</code>。</p>
</li>
<li><p>因此，我们必须把每个样本的 <code>(3, 10)</code> 这部分“铺平”，也就是将3个10维的向量拼接成1个30维的长向量。这样数据形状就从 <code>(32, 3, 10)</code> 变成了 <code>(32, 30)</code>，完美地符合了隐藏层的输入要求。</p>
</li>
</ul>
<h3 id="4-输出层：一个需要-уточнение-clarification-的小细节"><a href="#4-输出层：一个需要-уточнение-clarification-的小细节" class="headerlink" title="4. 输出层：一个需要 уточнение (clarification) 的小细节"></a>4. 输出层：一个需要 уточнение (clarification) 的小细节</h3><ol>
<li><p><strong>隐藏层激活 (tanh)</strong>：数据 <code>(32, 30)</code> 进入隐藏层，经过线性变换后，<strong>使用 <code>tanh</code> 函数进行激活</strong>。<code>tanh</code> 是作用在<strong>隐藏层</strong>的。</p>
</li>
<li><p><strong>输出层 (Logits)</strong>：<code>tanh</code> 的输出结果，再进入下一个线性层（图中最上面的蓝色粗框），计算出最终的得分（logits）。这个输出的形状是 <code>(32, 27)</code>。</p>
</li>
<li><p><strong>概率转换 (softmax)</strong>：<strong>最后一步</strong>，才是将这个 <code>(32, 27)</code> 的得分矩阵通过 <code>softmax</code> 函数，转换成概率分布。<code>softmax</code> 是作用在<strong>最终输出层</strong>之后，用来解释结果的。</p>
</li>
</ol>
<p>流程是： <strong>展平 -&gt; 隐藏层线性变换 -&gt; <code>tanh</code> 激活 -&gt; 输出层线性变换 -&gt; <code>softmax</code></strong>。</p>
<h1 id="Pytorch的强大之处-Fused-Kernel："><a href="#Pytorch的强大之处-Fused-Kernel：" class="headerlink" title="Pytorch的强大之处-Fused Kernel："></a>Pytorch的强大之处-Fused Kernel：</h1><p> <strong>Fused Kernel（融合计算核心）</strong> 是现代深度学习框架（如PyTorch, TensorFlow）和硬件（GPU）中一个至关重要的性能优化技术。</p>
<h3 id="厨房的比喻"><a href="#厨房的比喻" class="headerlink" title="厨房的比喻"></a>厨房的比喻</h3><ul>
<li><p>非融合操作 (Non-Fused Operation)：</p>
<p>  想象一下你在厨房做一道菜，需要三个步骤：1.切菜，2.炒菜，3.装盘。</p>
<p>  你先把所有菜都切好，然后把它们全部放回冰箱（慢速内存）。接着，你再从冰箱里把切好的菜拿出来，开始炒菜，炒好后又全部放回冰箱。最后，你再从冰箱里把炒好的菜拿出来，开始装盘。</p>
<p>  在这个过程中，你来回开关冰箱、存取食材花费了大量的时间。</p>
</li>
<li><p>融合操作 (Fused Operation / Fused Kernel)：</p>
<p>  现在，你把需要处理的菜放在手边的操作台（快速缓存/寄存器）上。你直接在操作台上完成切菜、炒菜，然后立刻装盘。整个过程一气呵成，几乎没有浪费时间去冰箱存取半成品。</p>
</li>
</ul>
<h3 id="Fused-Kernel-的技术解释"><a href="#Fused-Kernel-的技术解释" class="headerlink" title="Fused Kernel 的技术解释"></a>Fused Kernel 的技术解释</h3><p>在GPU计算中，“冰箱”就是速度较慢的<strong>全局内存（Global Memory）</strong>，“操作台”就是速度极快的<strong>片上缓存或寄存器（On-chip Cache/Registers）</strong>。</p>
<ol>
<li><p>Kernel (计算核心)：</p>
<p> 一个 “Kernel” 是指发送给GPU执行的一个独立的计算任务。例如，torch.exp(x) 会启动一个指数运算的Kernel，torch.sum(y) 会启动一个求和的Kernel。</p>
</li>
<li><p>Fused Kernel (融合计算核心)：</p>
<p> 它指的是将多个独立的、连续的计算步骤（Kernels）合并成一个单一的、更大的Kernel。</p>
</li>
</ol>
<p><code>F.cross_entropy</code> 这个函数就是使用了 Fused Kernel。我们来看看它融合了哪些操作：</p>
<p><strong>如果不使用 Fused Kernel，手动计算交叉熵损失需要以下步骤：</strong></p>
<ol>
<li><p>logits -&gt; Softmax</p>
<p> a. 对 logits 取指数 torch.exp() (启动第1个Kernel，生成一个中间结果)</p>
<p> b. 对指数结果按行求和 torch.sum() (启动第2个Kernel，又一个中间结果)</p>
<p> c. 用指数结果除以和 a / b (启动第3个Kernel，得到概率 probs)</p>
</li>
<li><p>probs -&gt; Negative Log Likelihood Loss</p>
<p> d. 对概率 probs 取对数 torch.log() (启动第4个Kernel，又一个中间结果)</p>
<p> e. 根据正确标签 y 提取对应的对数概率 (启动第5个Kernel)</p>
<p> f. 取负数并求平均值 (启动第6个Kernel)</p>
</li>
</ol>
<p>你看，这个过程不仅繁琐，而且每一步都会产生一个巨大的中间张量（比如 <code>probs</code>），这些张量需要被写入GPU的全局内存，然后在下一步再被读取出来。这种频繁的内存读写是<strong>非常耗时</strong>的。</p>
<p><strong>使用 <code>F.cross_entropy</code> (Fused Kernel) 的情况：</strong></p>
<p>你只需要调用一个函数，它会启动<strong>一个高度优化的 Kernel</strong>。这个Kernel在GPU内部，一口气完成上述所有的计算步骤。中间结果（如<code>probs</code>）尽可能地被保留在超高速的片上缓存中，而不需要写入慢速的全局内存。</p>
<h3 id="Fused-Kernel-的三大优势"><a href="#Fused-Kernel-的三大优势" class="headerlink" title="Fused Kernel 的三大优势"></a>Fused Kernel 的三大优势</h3><ol>
<li><p><strong>大幅减少内存读写 (Drastically Reduces Memory I/O)</strong>：这是最核心的优势。避免了生成和存储多个巨大的中间张量，从而显著减少了对慢速全局内存的访问次数，极大提升了计算速度。</p>
</li>
<li><p><strong>减少计算启动开销 (Reduces Kernel Launch Overhead)</strong>：CPU每次命令GPU启动一个Kernel都有微小的开销。启动1个大Kernel比启动6个小Kernel的总体开销要小得多。</p>
</li>
<li><p><strong>提升数值稳定性 (Improves Numerical Stability)</strong>：这也是视频中提到的关键点。在计算 <code>exp(logits)</code> 时，如果 <code>logits</code> 中的数值很大，结果可能会溢出变成 <code>inf</code>（无穷大），导致计算错误。融合后的Kernel可以使用一些数学技巧（如 Log-Sum-Exp Trick）来巧妙地避免这种溢出问题，使得计算在数值上更加稳定和精确。</p>
</li>
</ol>
<h1 id="交叉熵损失（Cross-Entropy-Loss）"><a href="#交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="交叉熵损失（Cross-Entropy Loss）"></a>交叉熵损失（Cross-Entropy Loss）</h1><h3 id="第一部分：Softmax-从-Logits-到概率"><a href="#第一部分：Softmax-从-Logits-到概率" class="headerlink" title="第一部分：Softmax (从 Logits 到概率)"></a><strong>第一部分：Softmax (从 Logits 到概率)</strong></h3><p> <strong>“把logit进行exp之后，归一化”</strong>，这个操作本身就叫 <strong>Softmax 函数</strong>。</p>
<ul>
<li><p><strong>目的</strong>：它的唯一目的就是将模型输出的一组任意分值的 <code>logits</code>（例如 <code>[-1.2, 3.4, 0.5]</code>），转换成一个规范的<strong>概率分布</strong>。</p>
</li>
<li><p><strong>特性</strong>：经过 Softmax 处理后，输出的向量有两个特点：</p>
<ol>
<li><p>所有元素都在 0 到 1 之间。</p>
</li>
<li><p>所有元素之和等于 1。</p>
</li>
</ol>
</li>
<li><p><strong>结果</strong>：这样我们就得到了模型预测的：“下一个字符是’a’的概率是10%，是’b’的概率是85%，是’c’的概率是5%…”</p>
</li>
</ul>
<h3 id="第二部分：负对数似然损失-Negative-Log-Likelihood-Loss"><a href="#第二部分：负对数似然损失-Negative-Log-Likelihood-Loss" class="headerlink" title="第二部分：负对数似然损失 (Negative Log Likelihood Loss)"></a><strong>第二部分：负对数似然损失 (Negative Log Likelihood Loss)</strong></h3><p>后续步骤 <strong>“将索引和标签输入概率矩阵，取log，取平均，取负号”</strong>，这部分合起来就是<strong>负对数似然损失</strong>。</p>
<ol>
<li><p><strong>挑选正确概率</strong>：我们从上面得到的概率分布中，只关心<strong>正确答案</strong>所对应的那个概率值。比如，如果正确答案是 ‘b’，我们就从 <code>[0.10, 0.85, 0.05]</code> 中挑选出 <code>0.85</code>。</p>
</li>
<li><p><strong>取 Log 再取负号 (<code>-log(p)</code>)</strong>：这是损失函数的核心。</p>
<ul>
<li><p>如果模型预测得<strong>非常准</strong>，正确答案的概率 <code>p</code> 接近 <code>1</code>（比如 <code>0.99</code>），那么 <code>-log(0.99)</code> 是一个<strong>非常小</strong>的数（接近0）。这意味着损失很小，惩罚很轻。</p>
</li>
<li><p>如果模型预测得非常差，正确答案的概率 p 接近 0（比如 0.01），那么 -log(0.01) 是一个非常大的数。这意味着损失很大，惩罚很重。</p>
<p>  这个特性完美地符合我们对一个损失函数的要求：预测越差，惩罚越重。</p>
</li>
</ul>
</li>
<li><p><strong>取平均</strong>：因为我们一次处理一个批次（batch）的样本（比如32个），我们会为这32个样本分别计算出损失值。最后，我们将这32个损失值加起来求一个平均，得到一个单一的数值，代表模型在这个批次上的总体表现。这个最终的数值就是我们用来进行反向传播、更新模型参数的依据。</p>
</li>
</ol>
<h1 id="minibatch加速"><a href="#minibatch加速" class="headerlink" title="minibatch加速"></a>minibatch加速</h1><h3 id="一、为什么需要-Batch？-动机"><a href="#一、为什么需要-Batch？-动机" class="headerlink" title="一、为什么需要 Batch？(动机)"></a><strong>一、为什么需要 Batch？(动机)</strong></h3><p>想象一下，你的整个训练数据集 <code>X</code> 有20多万个样本。训练模型需要计算损失函数对参数的梯度，以便更新参数。这里有两种极端的做法：</p>
<ol>
<li><p><strong>极端一：使用全部数据 (Full Batch)</strong></p>
<ul>
<li><p><strong>做法</strong>：一次性将20多万个样本全部喂给模型，计算一个总损失，然后进行一次反向传播和参数更新。</p>
</li>
<li><p><strong>缺点</strong>：计算量<strong>极其巨大</strong>。20多万个样本的前向传播和反向传播可能会耗尽你的内存，而且花费的时间会非常非常长。你可能要等几分钟甚至几小时才能完成<strong>一步</strong>更新。</p>
</li>
</ul>
</li>
<li><p><strong>极端二：一次只用一个数据 (Stochastic Gradient Descent, SGD)</strong></p>
<ul>
<li><p><strong>做法</strong>：一次只随机取一个样本，计算损失，更新一次参数。</p>
</li>
<li><p><strong>缺点</strong>：<strong>极其不稳定</strong>。单个样本带来的梯度具有很大的随机性，可能会让模型的训练过程像喝醉酒一样摇摇晃晃，虽然大方向对，但收敛速度会很慢。同时，这也没有充分利用GPU并行计算的优势。</p>
</li>
</ul>
</li>
</ol>
<p><strong>Minibatch (小批量) 就是介于两者之间的完美平衡点</strong>。我们一次取一小撮数据（例如32个），这一小撮数据就称为一个 “minibatch”。</p>
<ul>
<li><p><strong>优点</strong>：</p>
<ul>
<li><p><strong>计算高效</strong>：32个样本的计算量很小，一步更新非常快。</p>
</li>
<li><p><strong>梯度稳定</strong>：32个样本的平均梯度比单个样本的梯度要稳定得多，能更准确地指向正确的下降方向。</p>
</li>
<li><p><strong>充分利用硬件</strong>：可以高效地进行并行计算。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="二、代码是怎么实现-Batch-的？-图解"><a href="#二、代码是怎么实现-Batch-的？-图解" class="headerlink" title="二、代码是怎么实现 Batch 的？(图解)"></a><strong>二、代码是怎么实现 Batch 的？(图解)</strong></h3><p>现在我们来逐行看懂图中<code># minibatch construct</code>部分的代码，这正是“抓取一小撮数据”的过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 X 的总大小是 (228146, 3)</span></span><br><span class="line"><span class="comment"># 假设 Y 的总大小是 (228146,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># minibatch construct</span></span><br><span class="line">ix = torch.randint(<span class="number">0</span>, X.shape[<span class="number">0</span>], (<span class="number">32</span>,)) </span><br></pre></td></tr></table></figure>

<ol>
<li><p><strong><code>X.shape[0]</code></strong>: 这是获取我们完整数据集 <code>X</code> 的总样本数，也就是总行数，比如 <code>228146</code>。</p>
</li>
<li><p><strong><code>torch.randint(0, 228146, (32,))</code></strong>: 这是这行代码的核心。它的意思是：</p>
<ul>
<li><p>请在一个从 <code>0</code> 到 <code>228145</code> 的整数范围内…</p>
</li>
<li><p>…随机地、可重复地抽取…</p>
</li>
<li><p>…<code>32</code> 个整数。</p>
</li>
<li><p>执行后，<code>ix</code> 会变成一个包含32个随机数字的张量（Tensor），例如 <code>tensor([501, 189234, 98, ..., 76521])</code>。</p>
</li>
</ul>
<p> <strong><code>ix</code> 现在就是我们这一批（batch）随机选出来的样本的“门牌号”或者说“行索引”。</strong>
 </p>
</li>
<li><p><strong><code>emb = C[X[ix]]</code></strong> 和 <strong><code>loss = F.cross_entropy(logits, Y[ix])</code></strong></p>
<ul>
<li><p><strong><code>X[ix]</code></strong>: 这是利用 <code>ix</code> 来从整个数据集 <code>X</code> 中“取出”数据的关键一步。PyTorch/Numpy允许我们用一个索引列表（或张量）来一次性地选取多行数据。<code>X[ix]</code> 的结果就是一个新的、更小的张量，形状为 <code>(32, 3)</code>。这 <strong>32</strong> 行数据就是我们本次训练要用的 <strong>minibatch</strong>。</p>
</li>
<li><p><strong><code>Y[ix]</code></strong>: 同理，我们使用<strong>完全相同</strong>的索引 <code>ix</code> 从标签集 <code>Y</code> 中取出对应的32个正确答案。这保证了数据和标签是一一对应的。</p>
</li>
</ul>
</li>
</ol>
<h1 id="“Learning-Rate-Range-Test”（学习率范围测试）"><a href="#“Learning-Rate-Range-Test”（学习率范围测试）" class="headerlink" title="“Learning Rate Range Test”（学习率范围测试）"></a><strong>“Learning Rate Range Test”</strong>（学习率范围测试）</h1><h3 id="一、-核心思想：为什么学习率如此重要？"><a href="#一、-核心思想：为什么学习率如此重要？" class="headerlink" title="一、 核心思想：为什么学习率如此重要？"></a>一、 核心思想：为什么学习率如此重要？</h3><p>首先，我们要理解学习率（Learning Rate, LR）扮演的角色。在梯度下降中，我们计算出让损失（loss）减小的“方向”（梯度），然后沿着这个方向“走一步”来更新模型的参数。</p>
<p><strong>学习率就是你“这一步”迈得有多大（步长）。</strong></p>
<ul>
<li><p><strong>学习率太小 (Too Low)</strong>：你每一步都迈得小心翼翼，像在挪动。虽然方向是对的，但要走到谷底（损失最低点）会花费极长的时间，训练效率极低。</p>
</li>
<li><p><strong>学习率太大 (Too High)</strong>：你每一步都迈得大步流星。你可能一步就直接跨过了谷底，跳到了对面的山坡上。下一步你又想往回走，结果可能又跳了回来。最终结果就是在谷底两侧来回震荡，甚至越跳越远（损失爆炸，变成 <code>NaN</code>），永远无法收敛。</p>
</li>
</ul>
<p>因此，找到一个“不大不小刚刚好”的学习率，是模型训练成功与否的关键。</p>
<h3 id="二、-暴力搜索的问题"><a href="#二、-暴力搜索的问题" class="headerlink" title="二、 暴力搜索的问题"></a>二、 暴力搜索的问题</h3><p>最朴素的想法是：“我多试几个值不就行了？比如 0.1, 0.01, 0.001…”。</p>
<p>这种方法的问题在于：</p>
<ol>
<li><p><strong>盲目</strong>：你不知道最佳值到底在哪个数量级，可能试了很多次都错过了最佳范围。</p>
</li>
<li><p><strong>耗时</strong>：每次尝试都需要从头开始一次完整的训练，如果模型很大，试一次可能就要几个小时甚至几天，成本太高。</p>
</li>
</ol>
<h3 id="三、-高效技巧：学习率范围测试"><a href="#三、-高效技巧：学习率范围测试" class="headerlink" title="三、 高效技巧：学习率范围测试"></a>三、 高效技巧：学习率范围测试</h3><p>演示的方法，就是为了解决上述问题。它在一个<strong>单次、短暂的实验</strong>中，系统性地探索从极小到极大的学习率范围，并观察其对损失的影响，从而快速定位出最佳的学习率区间。<br><img src="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826080634031.png" alt="image.png"></p>
<h4 id="操作步骤-Step-by-Step"><a href="#操作步骤-Step-by-Step" class="headerlink" title="操作步骤 (Step-by-Step)"></a><strong>操作步骤 (Step-by-Step)</strong></h4><ol>
<li><p>设定一个学习率的变化范围</p>
<p> 我们不测试几个离散的点，而是测试一个连续变化的范围。关键在于，这个范围应该是对数尺度 (log scale) 的，因为我们更关心学习率的数量级（是 0.1 级别还是 0.01 级别）。</p>
<p> 在代码中，这通常通过 lrs = 10**torch.linspace(-3, 0, 1000) 实现。</p>
<ul>
<li><p><code>torch.linspace(-3, 0, 1000)</code>: 生成从 -3 到 0 的 1000 个等间距数字。</p>
</li>
<li><p><code>10**...</code>: 将上面的数字作为 10 的指数。</p>
</li>
<li><p>结果就是 <code>lrs</code> 包含了从 <code>10**-3</code> (0.001) 到 <code>10**0</code> (1.0) 的 1000 个平滑递增的学习率。</p>
</li>
</ul>
</li>
<li><p>进行一个短暂的“预训练”</p>
<p> 我们让模型开始训练，但不是用一个固定的学习率，而是在每一次迭代（或每一步）中，都从上面生成的 lrs 列表中按顺序取出一个新的、更大的学习率来更新参数。</p>
<ul>
<li><p>第1步更新，使用 <code>lr = 0.001</code></p>
</li>
<li><p>第2步更新，使用 <code>lr = 0.001007</code></p>
</li>
<li><p>…</p>
</li>
<li><p>第1000步更新，使用 lr = 1.0</p>
<p>  同时，在每一步，我们都记录下当前使用的学习率和该步计算出的损失值 (loss)。这个过程通常只需要几百或几千次迭代，非常快。</p>
</li>
</ul>
</li>
<li><p>绘制并解读“学习率 vs. 损失”曲线</p>
<p> 实验结束后，我们以学习率（X轴，对数坐标）和损失（Y轴）为坐标，将记录下的数据点绘制成图。你会得到一条非常典型的曲线，大致分为四个区域：<br> <img src="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826080722214.png" alt="image.png"></p>
<ul>
<li><p><strong>区域 A (平坦区)</strong>：学习率<strong>太小</strong>。损失几乎不下降，因为步子太小，模型基本没在学习。</p>
</li>
<li><p><strong>区域 B (急降区)</strong>：学习率进入了<strong>合适的范围</strong>。损失开始迅速、稳定地下降。这是我们最感兴趣的区域。</p>
</li>
<li><p><strong>区域 C (平原区)</strong>：学习率达到了<strong>最佳值附近</strong>。损失下降到最低点，并可能维持一小段。</p>
</li>
<li><p><strong>区域 D (爆炸区)</strong>：学习率<strong>太大</strong>。损失突然开始急剧上升，模型训练开始发散，变得不稳定。</p>
</li>
</ul>
</li>
</ol>
<h4 id="如何选择最佳学习率？"><a href="#如何选择最佳学习率？" class="headerlink" title="如何选择最佳学习率？"></a><strong>如何选择最佳学习率？</strong></h4><p>最关键的一步来了。看着这张图，你应该选择哪个值作为你正式训练的学习率呢？</p>
<ul>
<li><p><strong>错误的选择</strong>：直接选择让损失值最低的那个点（区域C的末端）。</p>
</li>
<li><p><strong>为什么错误</strong>：这个点位于“悬崖边上”，是模型能够承受的<strong>最大学习率</strong>。虽然它在当前这个短暂的实验中让损失下降最快，但在漫长的正式训练中，任何微小的数据波动都可能让它“一步踏空”，直接进入爆炸区，导致训练失败。这个学习率<strong>太激进</strong>了。</p>
</li>
<li><p><strong>正确且稳妥的选择 (Rule of Thumb)</strong>：</p>
<ol>
<li><p>找到损失开始爆炸前的<strong>最低点</strong>所在的学习率（比如图中的 <code>10**-1</code>，即 <code>0.1</code>）。</p>
</li>
<li><p>将这个值<strong>除以10</strong>。也就是 <code>0.1 / 10 = 0.01</code>（即 <code>10**-2</code>）。</p>
</li>
<li><p><strong>这个 <code>0.01</code> 就是一个非常棒的初始学习率！</strong></p>
</li>
</ol>
<p>  这个选择让你处于<strong>急降区的中间位置（区域B）</strong>。在这个位置，学习率足够大，可以保证损失快速下降（训练速度快），同时又远离“悬崖”，有足够的安全边际来保证整个训练过程的稳定性。</p>
</li>
</ul>
<h1 id="解读loss函数图"><a href="#解读loss函数图" class="headerlink" title="解读loss函数图"></a>解读loss函数图</h1><p><img src="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826080319533.png" alt="image.png"></p>
<h2 id="整体训练状态"><a href="#整体训练状态" class="headerlink" title="整体训练状态"></a>整体训练状态</h2><p>训练成功的迹象：</p>
<ol>
<li><p>明显的下降趋势：损失从约 1.4 快速下降到 0.2 左右</p>
</li>
<li><p>收敛稳定：在后期基本稳定在较低水平</p>
</li>
<li><p>无明显震荡：曲线相对平滑，没有剧烈波动</p>
</li>
</ol>
<h2 id="详细阶段分析"><a href="#详细阶段分析" class="headerlink" title="详细阶段分析"></a>详细阶段分析</h2><h3 id="阶段-1：快速下降期-0-25000-步"><a href="#阶段-1：快速下降期-0-25000-步" class="headerlink" title="阶段 1：快速下降期 (0-25000 步)"></a>阶段 1：快速下降期 (0-25000 步)</h3><ul>
<li><p>特征：损失急剧下降</p>
</li>
<li><p>含义：模型快速学习基本模式</p>
</li>
<li><p>学习率：0.1（较高），促进快速学习</p>
</li>
</ul>
<h3 id="阶段-2：平缓下降期-25000-100000-步"><a href="#阶段-2：平缓下降期-25000-100000-步" class="headerlink" title="阶段 2：平缓下降期 (25000-100000 步)"></a>阶段 2：平缓下降期 (25000-100000 步)</h3><ul>
<li><p>特征：下降速度放缓但持续</p>
</li>
<li><p>含义：模型细化对数据的理解</p>
</li>
<li><p>仍在学习：未完全收敛</p>
</li>
</ul>
<h3 id="阶段-3：稳定收敛期-100000-200000-步"><a href="#阶段-3：稳定收敛期-100000-200000-步" class="headerlink" title="阶段 3：稳定收敛期 (100000-200000 步)"></a>阶段 3：稳定收敛期 (100000-200000 步)</h3><ul>
<li><p>特征：损失趋于稳定，小幅震荡</p>
</li>
<li><p>学习率切换：从 0.1 降到 0.01</p>
</li>
<li><p>收敛状态：模型基本训练完成</p>
</li>
</ul>
<h1 id="词嵌入空间"><a href="#词嵌入空间" class="headerlink" title="词嵌入空间"></a>词嵌入空间</h1><p><img src="https://cdn.jsdelivr.net/gh/JAYLI19707/blog-pic@master/20250826081534156.png" alt="image.png"></p>
<h2 id="语言学模式发现"><a href="#语言学模式发现" class="headerlink" title="语言学模式发现"></a>语言学模式发现</h2><h3 id="1-元音字母聚类"><a href="#1-元音字母聚类" class="headerlink" title="1. 元音字母聚类"></a>1. 元音字母聚类</h3><p>观察：a, e, i, o, u, y 在图中相对聚集</p>
<ul>
<li><p>位置：主要分布在中心区域 (-0.5 到 0.5 范围)</p>
</li>
<li><p>含义：模型自动学习到了元音的相似性</p>
</li>
<li><p>语言学意义：元音在名字中有相似的功能和分布模式</p>
</li>
</ul>
<h3 id="2-特殊字符的独特位置"><a href="#2-特殊字符的独特位置" class="headerlink" title="2. 特殊字符的独特位置"></a>2. 特殊字符的独特位置</h3><p>结束符 .：</p>
<ul>
<li><p>位置：右上角 (约 2.5, 0.05)，远离其他字符</p>
</li>
<li><p>含义：模型学习到结束符的特殊性</p>
</li>
<li><p>功能：标记单词边界，与其他字符有本质不同</p>
</li>
</ul>
<h3 id="3-辅音字母的分散分布"><a href="#3-辅音字母的分散分布" class="headerlink" title="3. 辅音字母的分散分布"></a>3. 辅音字母的分散分布</h3><p>观察：辅音字母分布更加分散</p>
<ul>
<li><p>高频辅音：r, n, l, m 等相对靠近中心</p>
</li>
<li><p>低频辅音：j, q, x, z 等分布在边缘</p>
</li>
<li><p>含义：模型根据使用频率和上下文相似性进行了分组</p>
</li>
</ul>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Regression/"
      title="机器学习-Regression"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        机器学习-Regression
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BFinetuning%20vs.%20Prompting/"
      title="机器学习-大语言模型Finetuning vs. Prompting"
     >

    <p class="title-text">
      
        机器学习-大语言模型Finetuning vs. Prompting
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>





    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 Justin<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
