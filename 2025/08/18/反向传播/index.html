<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="true" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>反向传播 | Justin的技术博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="通过从零构建一个名为micrograd的微型自动求导引擎，来深入、直观地理解神经网络的训练核心——反向传播算法。 第一阶段：核心概念与导数直观理解 Micrograd 简介:  它是一个自动梯度（Autograd）引擎，其核心功能是实现反向传播（Backpropagation）。  反向传播是高效计算损失函数相对于神经网络所有权重和偏置的梯度（Derivatives）的算法。梯度指明了调整参数以减">
<meta property="og:type" content="article">
<meta property="og:title" content="反向传播">
<meta property="og:url" content="https://jayli19707.github.io/2025/08/18/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/index.html">
<meta property="og:site_name" content="Justin的技术博客">
<meta property="og:description" content="通过从零构建一个名为micrograd的微型自动求导引擎，来深入、直观地理解神经网络的训练核心——反向传播算法。 第一阶段：核心概念与导数直观理解 Micrograd 简介:  它是一个自动梯度（Autograd）引擎，其核心功能是实现反向传播（Backpropagation）。  反向传播是高效计算损失函数相对于神经网络所有权重和偏置的梯度（Derivatives）的算法。梯度指明了调整参数以减">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-17T21:01:52.000Z">
<meta property="article:modified_time" content="2025-08-17T21:02:47.461Z">
<meta property="article:author" content="Justin">
<meta property="article:tag" content="人工智能,机器学习,深度学习,量化交易,Python,技术博客">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Justin的技术博客" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  
  
    
<div id="banner" class="">
  <img src="/image/background/benjamin_background.jpg" itemprop="image">
  <div id="banner-dim"></div>
</div>
 
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>Justin的技术博客 </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/projects">Projects</a>
    
      <a class="main-nav-link" href="/investment">Investment</a>
    
      <a class="main-nav-link" href="/photography">Photography</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/projects">Projects</a>
    
      <a class="nav-dropdown-link" href="/investment">Investment</a>
    
      <a class="nav-dropdown-link" href="/photography">Photography</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/image/avatar/avatar_1.png></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">Justin </div>
      <div class="dot"></div>
      <div class="subtitle">比世界先发现你发光 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/JAYLI19707" title="GitHub"><i class="fa-brands fa-github"></i></a>
        
          <a class="link-btn" href="mailto:your.email@gmail.com" title="Email"><i class="fa-solid fa-envelope"></i></a>
        
          <a class="link-btn" href="/atom.xml" title="RSS"><i class="fa-solid fa-rss"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">
                计算机
                <div class="category-count">3</div>
            </a>
        
            <a class="category-link" href="/categories/Leetcode/">
                Leetcode
                <div class="category-count">19</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">
                开发工具
                <div class="category-count">1</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                机器学习
                <div class="category-count">18</div>
            </a>
        
            <a class="category-link" href="/categories/%E9%87%91%E8%9E%8D/">
                金融
                <div class="category-count">7</div>
            </a>
        
            <a class="category-link" href="/categories/Quant/">
                Quant
                <div class="category-count">2</div>
            </a>
        
            <a class="category-link" href="/categories/AI/">
                AI
                <div class="category-count">3</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">
                操作系统
                <div class="category-count">1</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%95%B0%E5%AD%A6/">
                数学
                <div class="category-count">3</div>
            </a>
        
            <a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">
                计算机网络
                <div class="category-count">1</div>
            </a>
        </div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">标签</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Batch/" rel="tag">Batch</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/LSTM/" rel="tag">LSTM</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Quant/" rel="tag">Quant</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Shell/" rel="tag">Shell</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E4%B8%B4%E8%BF%91%E7%AE%97%E5%AD%90/" rel="tag">临近算子</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E7%B1%BB/" rel="tag">二分类</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%87%B8%E5%87%BD%E6%95%B0/" rel="tag">凸函数</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/" rel="tag">最大似然估计</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%9F%E6%9D%83/" rel="tag">期权</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">梯度下降</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/" rel="tag">矩阵乘法</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/" rel="tag">统计假设检验</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" rel="tag">计算机网络</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%87%8F%E4%BB%B7%E8%B6%8B%E5%8A%BF/" rel="tag">量价趋势</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6/" rel="tag">金融数学</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" rel="tag">面试题</a></li></ul>
    </div>
  </div>


    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-反向传播" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        反向传播
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-08-17T21:01:52.000Z" itemprop="datePublished">2025-08-18</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            7.7k 词 
          </div>
        </div>
        
      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <p>通过从零构建一个名为<code>micrograd</code>的微型自动求导引擎，来深入、直观地理解神经网络的训练核心——反向传播算法。</p>
<h1 id="第一阶段：核心概念与导数直观理解"><a href="#第一阶段：核心概念与导数直观理解" class="headerlink" title="第一阶段：核心概念与导数直观理解"></a>第一阶段：核心概念与导数直观理解</h1><ul>
<li><p><strong>Micrograd 简介</strong>:</p>
<ul>
<li><p>它是一个自动梯度（Autograd）引擎，其核心功能是实现<strong>反向传播（Backpropagation）</strong>。</p>
</li>
<li><p>反向传播是高效计算损失函数相对于神经网络所有权重和偏置的<strong>梯度</strong>（Derivatives）的算法。梯度指明了调整参数以减小损失的方向。</p>
</li>
<li><p>这是 PyTorch、JAX 等现代深度学习框架的数学核心。</p>
</li>
</ul>
</li>
<li><p><strong>核心演示</strong>:</p>
<ul>
<li><p><code>Value</code>对象：<code>micrograd</code>中的基本数据单元，可以构建数学表达式图。</p>
</li>
<li><p><strong>前向传播 (Forward Pass)</strong>：从输入开始，通过计算图计算出最终的输出值。</p>
</li>
<li><p><strong>反向传播 (Backward Pass)</strong>：在最终输出上调用<code>.backward()</code>，<code>micrograd</code>会自动、递归地应用<strong>链式法则（Chain Rule）</strong>，计算出最终输出对每一个输入和中间变量的梯度。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Value</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,data,_children=(<span class="params"></span>),_op=<span class="string">""</span>,label=<span class="string">""</span></span>):</span><br><span class="line"> </span><br><span class="line">        <span class="variable language_">self</span>.data=data    <span class="comment">#data: 存储节点的值</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.grad=<span class="number">0.0</span>      <span class="comment">#grad: 存储梯度</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._prev=<span class="built_in">set</span>(_children) <span class="comment">#_prev: 存储节点的父节点</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._op=_op       <span class="comment">#_op: 存储节点的操作符</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.label=label   <span class="comment">#label: 存储节点的标签</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._backward=<span class="keyword">lambda</span>:<span class="literal">None</span> <span class="comment">#_backward: 存储梯度计算的函数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"Value(data=<span class="subst">{self.data}</span>,grad=<span class="subst">{self.grad}</span>)"</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self,other</span>):</span><br><span class="line"></span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line"></span><br><span class="line">        out=Value(<span class="variable language_">self</span>.data+other.data,(<span class="variable language_">self</span>,other),<span class="string">"+"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out          </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self,other</span>):</span><br><span class="line"></span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line"></span><br><span class="line">        out=Value(<span class="variable language_">self</span>.data*other.data,(<span class="variable language_">self</span>,other),<span class="string">"*"</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__pow__</span>(<span class="params">self,other</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(other, (<span class="built_in">int</span>, <span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line">        out=Value(<span class="variable language_">self</span>.data**other,(<span class="variable language_">self</span>,),<span class="string">f"**<span class="subst">{other}</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>导数的意义</strong>:</p>
<ul>
<li><p>导数（梯度）衡量的是一种<strong>敏感度</strong>。例如，<code>a.grad = 138</code> 意味着，如果输入<code>a</code>的值发生一个微小的正向变动，最终输出会以138倍的斜率增加。</p>
</li>
<li><p>神经网络训练就是利用这个敏感度信息来微调参数，使得损失函数的输出向着减小的方向移动。</p>
</li>
</ul>
</li>
<li><p><strong>构建<code>Value</code>类</strong>:</p>
<ul>
<li><p>为了构建计算图，<code>Value</code>对象不仅存储数据，还记录了<code>_prev</code>（它的前驱节点/子节点）和<code>_op</code>（生成它的运算符号）。</p>
</li>
<li><p>通过重载Python的<code>__add__</code>和<code>__mul__</code>等方法，使得<code>Value</code>对象可以像普通数字一样进行运算，并自动构建计算图。</p>
</li>
<li><p>使用<code>draw_dot</code>函数可以可视化这个计算图，清晰地看到数据流。</p>
</li>
</ul>
</li>
</ul>
<h1 id="第二阶段：手动反向传播与链式法则"><a href="#第二阶段：手动反向传播与链式法则" class="headerlink" title="第二阶段：手动反向传播与链式法则"></a><strong>第二阶段：手动反向传播与链式法则</strong></h1><ul>
<li><p><strong>这是理解反向传播最核心的部分。</strong></p>
</li>
<li><p><strong>梯度初始化</strong>:</p>
<ul>
<li><p>在<code>Value</code>对象中增加<code>grad</code>属性，初始为0。</p>
</li>
<li><p>对于最终的输出节点（例如<code>L</code>），其<code>grad</code>被初始化为<code>1.0</code>，因为任何变量对自身的导数都是1 (<code>dL/dL = 1</code>)。</p>
</li>
</ul>
</li>
<li><p><strong>手动计算梯度</strong>:</p>
<ul>
<li><p><strong>从后向前</strong>，逐个节点计算梯度。</p>
</li>
<li><p><strong>加法节点 (+)</strong>：像一个“路由器”，它将上游传来的梯度<strong>原封不动地分配</strong>给它的所有输入。因为 <code>d(a+b)/da = 1</code>，所以梯度传递时乘以1，保持不变。</p>
</li>
<li><p><strong>乘法节点 (*)</strong>：像一个“交换机”。对于 <code>c = a * b</code>，<code>dc/da = b</code>，<code>dc/db = a</code>。因此，上游传来的梯度会分别乘以<strong>另一个输入的值</strong>，再传递给当前输入。<code>a.grad += L.grad * b.data</code>，<code>b.grad += L.grad * a.data</code>。</p>
</li>
<li><p><strong>链式法则 (Chain Rule)</strong>：是这一切的数学基础。如果 <code>z</code> 依赖于 <code>y</code>，<code>y</code> 依赖于 <code>x</code>，那么 <code>dz/dx = dz/dy * dy/dx</code>。反向传播就是链式法则在整个计算图上的递归应用。</p>
</li>
<li><p><strong>梯度累加</strong>: 如果一个变量在计算图中被多次使用，它的梯度需要<strong>累加（+=）</strong>，而不是直接赋值。这是一个非常关键且容易出错的细节。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self,other</span>):</span><br><span class="line"></span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line"></span><br><span class="line">        out=Value(<span class="variable language_">self</span>.data+other.data,(<span class="variable language_">self</span>,other),<span class="string">"+"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():        <span class="comment"># 没有 def 就会立即执行梯度计算。</span></span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.grad+=out.grad</span><br><span class="line"></span><br><span class="line">            other.grad+=out.grad</span><br><span class="line"></span><br><span class="line">        out._backward=_backward <span class="comment"># def 把梯度计算延迟到调用 backward() 时执行</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out              <span class="comment">#确保梯度计算的正确顺序。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self,other</span>):</span><br><span class="line"></span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line"></span><br><span class="line">        out=Value(<span class="variable language_">self</span>.data*other.data,(<span class="variable language_">self</span>,other),<span class="string">"*"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.grad+=out.grad*other.data</span><br><span class="line"></span><br><span class="line">            other.grad+=out.grad*<span class="variable language_">self</span>.data</span><br><span class="line"></span><br><span class="line">        out._backward=_backward</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__pow__</span>(<span class="params">self,other</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""举例：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            乘法: a * b - 两个都是变量，都需要梯度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            a = Value(2.0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            b = Value(3.0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            c = a * b → 父节点是 (a, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            幂运算: a ** 2 - 只有底数是变量，指数是常数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            a = Value(2.0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            c = a ** 2 → 父节点只有 (a,),因为指数2不需要梯度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(other, (<span class="built_in">int</span>, <span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line">        out=Value(<span class="variable language_">self</span>.data**other,(<span class="variable language_">self</span>,),<span class="string">f"**<span class="subst">{other}</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.grad+=out.grad*other*<span class="variable language_">self</span>.data**(other-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        out._backward=_backward</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        out = Value(<span class="number">0</span> <span class="keyword">if</span> <span class="variable language_">self</span>.data &lt; <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.data, (<span class="variable language_">self</span>,), <span class="string">'ReLU'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.grad += (out.data &gt; <span class="number">0</span>) * out.grad</span><br><span class="line"></span><br><span class="line">        out._backward = _backward</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="第三阶段：自动化反向传播与构建神经网络"><a href="#第三阶段：自动化反向传播与构建神经网络" class="headerlink" title="**第三阶段：自动化反向传播与构建神经网络 **"></a>**第三阶段：自动化反向传播与构建神经网络 **</h1><ul>
<li><p><strong>自动化 <code>_backward</code> 函数</strong>:</p>
<ul>
<li><p>为每一种运算（加、乘、Tanh激活函数等）定义一个局部的<code>_backward</code>函数。这个函数知道如何将输出的梯度传播给输入。</p>
</li>
<li><p>例如，Tanh的导数是 <code>1 - tanh(x)²</code>。它的<code>_backward</code>函数就会将上游梯度乘以 <code>1 - self.data²</code> 再传递下去。</p>
</li>
</ul>
</li>
<li><p><strong>拓扑排序 (Topological Sort)</strong>:</p>
<ul>
<li>为了确保反向传播按正确的顺序（从输出到输入）进行，需要对计算图进行拓扑排序。这保证了在计算一个节点的梯度时，所有依赖它的下游节点的梯度都已经计算完毕。</li>
</ul>
</li>
<li><p><strong>完整的 <code>backward()</code> 方法</strong>:</p>
<ul>
<li>这个方法封装了整个流程：1. 拓扑排序；2. 初始化最终节点的梯度为1；3. 反向遍历排序后的列表，依次调用每个节点的<code>_backward</code>函数。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        反向传播方法：从当前节点开始，自动计算整个计算图的梯度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        工作原理：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        1. 使用拓扑排序确保梯度计算的正确顺序</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        2. 将当前节点的梯度设为1.0（作为反向传播的起点）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        3. 按拓扑排序的逆序遍历所有节点，调用每个节点的_backward()方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        只要在任意节点调用.backward()，该节点之前（上游）的所有节点都会被自动计算完梯度！</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        这就是自动微分的核心思想：一次调用就能得到整个计算图的梯度。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        如果没有这步只能一个一个节点按._backward(),得到上一个节点的梯度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        topo=[]</span><br><span class="line"></span><br><span class="line">        visited=<span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">build_topo</span>(<span class="params">v</span>):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line"></span><br><span class="line">                visited.add(v)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> child <span class="keyword">in</span> v._prev:</span><br><span class="line"></span><br><span class="line">                    build_topo(child)</span><br><span class="line"></span><br><span class="line">                topo.append(v)</span><br><span class="line"></span><br><span class="line">        build_topo(<span class="variable language_">self</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.grad=<span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> <span class="built_in">reversed</span>(topo):</span><br><span class="line"></span><br><span class="line">            node._backward()</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>构建神经网络模块</strong>:</p>
<ul>
<li><p><strong>Neuron (神经元)</strong>：一个基本的计算单元，包含一组权重 <code>w</code> 和一个偏置 <code>b</code>。它对输入执行 <code>w*x + b</code> 的线性变换，然后通过一个非线性的激活函数（如Tanh）。</p>
</li>
<li><p><strong>Layer (层)</strong>：由多个独立的神经元组成。</p>
</li>
<li><p><strong>MLP (多层感知机)</strong>：将多个层按顺序堆叠起来，构成一个完整的神经网络。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>:</span><br><span class="line"></span><br><span class="line">    <span class="string">"""神经网络模块的基类"""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">zero_grad</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""将所有参数的梯度清零"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line"></span><br><span class="line">            p.grad = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parameters</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""返回模块的所有参数，基类默认返回空列表"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">"""单个神经元类"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nin, nonlin=<span class="literal">True</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        初始化神经元</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nin: 输入维度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nonlin: 是否使用非线性激活函数(ReLU)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机初始化权重</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.w = [Value(random.uniform(-<span class="number">1</span>,<span class="number">1</span>)) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nin)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 偏置初始化为0</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.b = Value(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否使用非线性激活</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.nonlin = nonlin</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""前向传播计算"""</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算加权和加偏置</span></span><br><span class="line"></span><br><span class="line">        act = <span class="built_in">sum</span>((wi*xi <span class="keyword">for</span> wi,xi <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.w, x)), <span class="variable language_">self</span>.b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据nonlin决定是否应用ReLU激活函数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> act.relu() <span class="keyword">if</span> <span class="variable language_">self</span>.nonlin <span class="keyword">else</span> act</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parameters</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""返回神经元的所有参数(权重+偏置)"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w + [<span class="variable language_">self</span>.b]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""字符串表示"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"<span class="subst">{<span class="string">'ReLU'</span> <span class="keyword">if</span> self.nonlin <span class="keyword">else</span> <span class="string">'Linear'</span>}</span>Neuron(<span class="subst">{<span class="built_in">len</span>(self.w)}</span>)"</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Layer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">"""神经网络层类，包含多个神经元""</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __init__(self, nin, nout, **kwargs):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        初始化层</span><br><span class="line"></span><br><span class="line">        nin: 输入维度</span><br><span class="line"></span><br><span class="line">        nout: 输出维度(神经元数量)</span><br><span class="line"></span><br><span class="line">        **kwargs: 传递给神经元的其他参数</span><br><span class="line"></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 创建nout个神经元</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __call__(self, x):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span>前向传播<span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 每个神经元都处理相同的输入</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        out = [n(x) for n in self.neurons]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 如果只有一个输出，直接返回值而不是列表</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        return out[0] if len(out) == 1 else out</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def parameters(self):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span>返回层中所有神经元的参数<span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        return [p for n in self.neurons for p in n.parameters()]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __repr__(self):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span>字符串表示<span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        return f"Layer of [{', '.join(str(n) for n in self.neurons)}]"</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">"""多层感知机(Multi-Layer Perceptron)"""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nin, nouts</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        初始化MLP</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nin: 输入维度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nouts: 各层的输出维度列表</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建层尺寸列表: [输入维度, 第一层输出, 第二层输出, ...]</span></span><br><span class="line"></span><br><span class="line">        sz = [nin] + nouts</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建各层，最后一层不使用非线性激活函数</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.layers = [Layer(sz[i], sz[i+<span class="number">1</span>], nonlin=i!=<span class="built_in">len</span>(nouts)-<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nouts))]</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""前向传播，逐层处理"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line"></span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parameters</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""返回所有层的参数"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [p <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers <span class="keyword">for</span> p <span class="keyword">in</span> layer.parameters()]</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">"""字符串表示"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"MLP of [<span class="subst">{<span class="string">', '</span>.join(<span class="built_in">str</span>(layer) <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers)}</span>]"</span></span><br></pre></td></tr></table></figure>


<h1 id="第四阶段：完整的神经网络训练流程"><a href="#第四阶段：完整的神经网络训练流程" class="headerlink" title="**第四阶段：完整的神经网络训练流程 **"></a>**第四阶段：完整的神经网络训练流程 **</h1><ul>
<li><p><strong>训练循环 (Training Loop)</strong> 是神经网络学习的核心，包含以下五个步骤：</p>
<ol>
<li><p><strong>前向传播 (Forward Pass)</strong>：将输入数据喂给神经网络，计算出预测值 <code>ypred</code>。</p>
</li>
<li><p><strong>计算损失 (Compute Loss)</strong>：使用一个<strong>损失函数</strong>（如均方误差MSE）来衡量预测值 <code>ypred</code> 和真实目标 <code>y_true</code> 之间的差距。损失是一个标量，代表了模型当前的“糟糕”程度。</p>
</li>
<li><p><strong>梯度清零 (Zero Grad)</strong>：<strong>极其重要的一步！</strong> 在每次反向传播之前，必须将所有参数（权重和偏置）的梯度重置为0。因为梯度是累加的，如果不清零，之前的梯度会干扰本次的计算。</p>
</li>
<li><p><strong>反向传播 (Backward Pass)</strong>：调用 <code>loss.backward()</code>，计算出损失对网络中每一个参数的梯度。</p>
</li>
<li><p><strong>参数更新 (Update Parameters)</strong>：根据梯度信息更新每一个参数。公式为：<code>参数.data += -学习率 * 参数.grad</code>。</p>
<ul>
<li><p><strong>学习率 (Learning Rate)</strong>：一个超参数，控制每次更新的步长。</p>
</li>
<li><p><strong>负号</strong>表示我们希望向着<strong>梯度下降</strong>的方向移动，从而最小化损失。</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>通过成千上万次重复这个循环，神经网络的参数会被微调，使得损失逐渐降低，模型的预测能力越来越强。</p>
</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="**总结 **"></a>**总结 **</h1><ul>
<li><p><strong>神经网络的本质</strong>: 它们是可微调的复杂数学表达式。</p>
</li>
<li><p><strong>训练的本质</strong>: 通过<strong>梯度下降</strong>算法，找到一组最优的参数（权重和偏置），使得损失函数最小化。</p>
</li>
<li><p><strong>核心引擎</strong>: <strong>反向传播</strong>是高效计算梯度的关键，它使得在巨大的参数空间中进行梯度下降成为可能。</p>
</li>
<li><p><strong>Micrograd 与 PyTorch</strong>: Micrograd 的API设计与PyTorch高度相似，理解了Micrograd的原理，就能更好地理解PyTorch等工业级框架的内部工作机制。</p>
</li>
</ul>
<p>**<strong>神经网络是一个巨大的数学表达式，我们通过反向传播计算梯度，再通过梯度下降来最小化损失，最终让这个表达式学会我们希望它完成的任务。</strong></p>
<p>希望这份笔记能够帮助你拨开神经网络的迷雾，真正理解深度学习的魅力所在。</p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5(%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BA%93)/"
      title="机器学习概念(面试题库)"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        机器学习概念(面试题库)
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/08/16/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89/"
      title="上下文工程（Context Engineering）"
     >

    <p class="title-text">
      
        上下文工程（Context Engineering）
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>





    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 Justin<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
